{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCNXRfU_lZLz",
        "outputId": "d298df5f-840f-44c8-e3e4-b9c88d360463"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "UoruuMp6o1Tx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "1iaghYqko2y0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
        "SIMILARITY_THRESHOLD = 0.9  # Threshold for determining plagiarism"
      ],
      "metadata": {
        "id": "myhLQk6Uo4KS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text by removing punctuation, numbers, and converting to lowercase\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    return text.lower().strip()  # Convert to lowercase and strip whitespace"
      ],
      "metadata": {
        "id": "FVna3ctRo7wo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file\n",
        "def load_dataset(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        logging.error(f\"Dataset file {file_path} does not exist.\")\n",
        "        return None\n",
        "    try:\n",
        "        dataset = pd.read_csv(file_path)\n",
        "        # Check if the required columns exist\n",
        "        if 'text' not in dataset.columns or 'filename' not in dataset.columns:\n",
        "            logging.error(\"Dataset must have 'filename' and 'text' columns.\")\n",
        "            return None\n",
        "        dataset['text'] = dataset['text'].apply(preprocess_text)  # Preprocess the text\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading dataset: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "faJkpX4Ko-IK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find plagiarism using FAISS for fast similarity search\n",
        "def find_plagiarism_faiss(filenames, embeddings):\n",
        "    dimension = embeddings.shape[1]  # Get the dimension of the embeddings\n",
        "    index = faiss.IndexFlatIP(dimension)  # Create a FAISS index for inner product search\n",
        "    faiss.normalize_L2(embeddings)  # Normalize embeddings for cosine similarity\n",
        "    index.add(embeddings)  # Add embeddings to the index\n",
        "\n",
        "    # Perform similarity search for all documents\n",
        "    _, indices = index.search(embeddings, len(filenames))\n",
        "\n",
        "    plagiarism_results = set()\n",
        "    for i, neighbors in enumerate(indices):\n",
        "        for j in neighbors:\n",
        "            if i < j:  # Avoid duplicate comparisons\n",
        "                similarity = np.dot(embeddings[i], embeddings[j])  # Calculate cosine similarity\n",
        "                if similarity >= SIMILARITY_THRESHOLD:  # Check if similarity exceeds the threshold\n",
        "                    file_a = filenames[i]\n",
        "                    file_b = filenames[j]\n",
        "                    plagiarism_results.add((file_a, file_b, similarity))\n",
        "    return plagiarism_results"
      ],
      "metadata": {
        "id": "gFh18WCwpAuK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save plagiarism results to a CSV file\n",
        "def save_results_to_csv(results, filename=\"plagiarism_results.csv\"):\n",
        "    df = pd.DataFrame(results, columns=[\"File A\", \"File B\", \"Similarity Score\"])\n",
        "    df['Similarity Score'] = df['Similarity Score'].apply(lambda x: round(x, 2))  # Round similarity scores\n",
        "    df.to_csv(filename, index=False)  # Save results to a CSV file\n",
        "    logging.info(f\"Results saved to {filename}.\")"
      ],
      "metadata": {
        "id": "bzMqg2w7pCDL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print plagiarism results in a readable format\n",
        "def print_results(results):\n",
        "    print(\"\\nPlagiarism Results:\")\n",
        "    for file_a, file_b, similarity in sorted(results, key=lambda x: x[2], reverse=True):\n",
        "        print(f\"{file_a} and {file_b} have a similarity score of {similarity:.2f}\")"
      ],
      "metadata": {
        "id": "YMkLS1V5pDsO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the plagiarism detection\n",
        "def main():\n",
        "    dataset_path = \"plagiarism_dataset.csv\"  # Path to the dataset file\n",
        "    dataset = load_dataset(dataset_path)  # Load the dataset\n",
        "    if dataset is None:\n",
        "        return\n",
        "    filenames = dataset['filename'].tolist()  # Get the list of filenames\n",
        "    documents = dataset['text'].tolist()  # Get the list of document texts\n",
        "\n",
        "    model = SentenceTransformer(MODEL_NAME)  # Load the pre-trained model\n",
        "    embeddings = model.encode(documents, convert_to_tensor=False)  # Encode documents into embeddings\n",
        "\n",
        "    plagiarism_results = find_plagiarism_faiss(filenames, embeddings)  # Detect plagiarism\n",
        "    print_results(plagiarism_results)  # Print the results\n",
        "    save_results_to_csv(plagiarism_results)  # Save the results to a CSV file\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqlDsKO0pH_k",
        "outputId": "111b8c8b-3ec2-4ec4-ad01-1ff3585656c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Plagiarism Results:\n",
            "doc1.txt and doc2.txt have a similarity score of 0.97\n",
            "doc3.txt and doc4.txt have a similarity score of 0.94\n"
          ]
        }
      ]
    }
  ]
}